# QUESTION DESCRIPTION 

* On E-commerce sites, the quality of product listing is crucial for improving search relevance and gaining customer attention. 
In this competition, you are provided a set of product names, description, and attributes,
as well as the quality score of their listing as rated by real customers.
You are challenged to build a model to automatically predict the quality of another set of product listings.

# CLEAN DATA

Using function in pandas read csv file. Using replace()function with regular expression to clean data.
```javascript
def load(file_name):
    df_test = pd.read_csv(file_name)
    sLength = len(df_test['id'])
    df_test.replace(regex={'<.*?>|&nbsp|\W': ' '}, inplace=True)
    df_test = df_test.fillna("miss")
```
 And combining all the column in the csv file except price. 
```javascript
import pandas as pd
X_train =loadX('train_data.csv')
X_test = load('test_data.csv')
df_train =pd.DataFrame(data=X_train['lvl1']+X_train['lvl2']+X_train['lvl3']+X_train['type']+X_train['name']+X_train['descrption'],
columns=['train'])
X_train_text = df_train.train
```
Because I combine all the data from different columns, So I can get all word from the file , which means I can use more word to form dictionary to tokenize data in every place.

# FEATURE DATA

Using keras, tensorflow model, using the model to train the data, which means keras and tensorflow can automaticaly recognize and differnate the feature.

# Model

The model contains a lot of layer which contain Embeding , GlobalAveragePooling1D, Dense , Dropout layer.
The reason I use sigmoid is because I hope the data can distribute follow the trend of sigmoid.
The link of picture of [sigmoid](https://en.wikipedia.org/wiki/Sigmoid_function#/media/File:Gjl-t(x).svg).

```javascript
import tensorflow as tf

model = keras.Sequential()
model.add(keras.layers.Embedding(20000, 16))

model.add(keras.layers.GlobalAveragePooling1D())
model.add(keras.layers.Dense(32, activation=tf.nn.relu))
model.add(keras.layers.Dropout(rate=0.5))
model.add(keras.layers.Dense(1, activation=tf.nn.sigmoid))

model.compile(optimizer=tf.train.AdamOptimizer(),
              loss='binary_crossentropy',
              metrics=['accuracy'])


model.fit(X_train_hh[2001:],y_score[2001:],
          epochs=11,
          batch_size=128,
          validation_data=(X_train_hh[0:2000], X_train[0:2000].score.values),
          verbose=2)

output_array = model.predict(X_test_hh)e model.
```

# experience
* The reason cause the success of model is I use all the data from all column to tokenize. 
At first , I think description and name is the most important feature, But the performance of model using only description and name is too bad. Then , I tried to add other column I found LVL1, LVL2, LVL3 and Type is more influence, But Only using LVL1, LVL2, LVL3 and Type will lead another problem which is  hard to differentiate the data tokenized. So the Only the model which has some noise will has better performance.The most important thing is you need to know the model can not be too precise and too general.
*  If do more job using word2vector or other way to deal with data, to find the relation between data, I can make the model better.
  I found word2vector can make model get understanding of word, which means the model will be more precise.
  Beside this, I can use bootstrap to combine multi model, which can train the data using different model, using different weight.
  Because, not all the data will be fit to train in the same model. Like we can use different model to train price and other model to train LVL1, LVL2, LVL3, then combine the different model to get the result.




